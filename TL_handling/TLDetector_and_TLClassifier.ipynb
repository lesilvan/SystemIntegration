{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND Object Detection Lab\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from PIL import Image\n",
    "#from PIL import ImageDraw\n",
    "#from PIL import ImageColor\n",
    "import time\n",
    "#from scipy.stats import norm\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "os.chdir('C:\\\\Users\\\\Silvan\\\\Documents\\\\Udacity_Nanodegree\\\\Project 9_Autonomous Vehicle\\\\Capstone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Read in info about raw data from car camera and safe in df (pd.DataFrame format)\n",
    "with open('images/images/classes.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    content = list()\n",
    "    for row in spamreader:\n",
    "        #print(', '.join(row))\n",
    "        row[0] = 'images/images/'+row[0]\n",
    "        content.append(row)\n",
    "\n",
    "with open('images/images2/classes.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        #print(', '.join(row))\n",
    "        row[0] = 'images/images2/'+row[0]\n",
    "        content.append(row)\n",
    "#df = pd.DataFrame(data=content, columns=[\"path\", \"class\"])\n",
    "#df[\"class\"].value_counts(),df.describe()\n",
    "content = np.array(content)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = list()\n",
    "with open('images/signlabels.csv', newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        content.append(row)\n",
    "content = np.array(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293, 600, 800, 3) uint8\n",
      "(293,) int32\n"
     ]
    }
   ],
   "source": [
    "def load_content(content, verbose=1):\n",
    "    images = list()\n",
    "    labels = list()\n",
    "    paths = list()\n",
    "    for path, label in content:\n",
    "        paths.append(path)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img)\n",
    "        if label == \"green\":\n",
    "            labels.append(2)\n",
    "        elif label == \"red\":\n",
    "            labels.append(0)\n",
    "        elif label == \"yellow\":\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(4)\n",
    "            print(\"[ERROR] Label of image \", path, \" neither red, yellow, or green!\")\n",
    "    images = np.asarray(images)\n",
    "    labels = np.asarray(labels)\n",
    "    if verbose ==1:\n",
    "        return images, labels\n",
    "    elif verbose ==2:\n",
    "        return images, labels, paths\n",
    "\n",
    "\n",
    "images, labels = load_content(content)\n",
    "print(images.shape, images.dtype)\n",
    "print(labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Inference\n",
    "\n",
    "In this part of the lab traffic light detection is using pretrained object detection models. The source of the used models was the [model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md), where one can download the relevant models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen inference graph files. NOTE: change the path to where you saved the models.\n",
    "#SSD_GRAPH_FILE = 'models/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "#RFCN_GRAPH_FILE = 'models/rfcn_resnet101_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "#FASTER_RCNN_GRAPH_FILE = 'models/faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017/frozen_inference_graph.pb'\n",
    "#FASTER_RCNN_KITTI_FILE = 'models/faster_rcnn_resnet101_kitti_2018_01_28/frozen_inference_graph.pb'\n",
    "SSDLITE_GRAPH_FILE = 'models/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are utility functions. The main purpose of these is to draw the bounding boxes back onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors (one for each class)\n",
    "#cmap = ImageColor.colormap\n",
    "#print(\"Number of colors =\", len(cmap))\n",
    "#COLOR_LIST = sorted([c for c in cmap.keys()])\n",
    "\n",
    "#\n",
    "# Utility funcs\n",
    "#\n",
    "def filter_boxes(min_score, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score:\n",
    "            idxs.append(i)\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def filter_boxes1(min_score, search_class, boxes, scores, classes):\n",
    "    \"\"\"Return boxes with a confidence >= `min_score`\"\"\"\n",
    "    n = len(classes)\n",
    "    idxs = []\n",
    "    for i in range(n):\n",
    "        if scores[i] >= min_score and classes[i] == search_class:\n",
    "            idxs.append(i)\n",
    "            break\n",
    "    \n",
    "    filtered_boxes = boxes[idxs, ...]\n",
    "    filtered_scores = scores[idxs, ...]\n",
    "    filtered_classes = classes[idxs, ...]\n",
    "    return filtered_boxes, filtered_scores, filtered_classes\n",
    "\n",
    "def to_image_coords(boxes, height, width):\n",
    "    \"\"\"\n",
    "    The original box coordinate output is normalized, i.e [0, 1].\n",
    "    \n",
    "    This converts it back to the original coordinate based on the image\n",
    "    size.\n",
    "    \"\"\"\n",
    "    box_coords = np.zeros_like(boxes)\n",
    "    box_coords[:, 0] = boxes[:, 0] * height\n",
    "    box_coords[:, 1] = boxes[:, 1] * width\n",
    "    box_coords[:, 2] = boxes[:, 2] * height\n",
    "    box_coords[:, 3] = boxes[:, 3] * width\n",
    "    \n",
    "    return box_coords\n",
    "\n",
    "def draw_boxes(image_path, boxes, classes, thickness=4):\n",
    "    \"\"\"Draw bounding boxes on the image\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for i in range(len(boxes)):\n",
    "        bot, left, top, right = boxes[i, ...]\n",
    "        class_id = int(classes[i])\n",
    "        color = COLOR_LIST[class_id]\n",
    "        draw.line([(left, top), (left, bot), (right, bot), (right, top), (left, top)], width=thickness, fill=color)\n",
    "        \n",
    "def load_graph(graph_file):\n",
    "    \"\"\"Loads a frozen inference graph\"\"\"\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(graph_file, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_tl(image, sess):\n",
    "    image_np = np.expand_dims(np.asarray(image, dtype=np.uint8), 0)\n",
    "    (boxes, scores, classes) = sess.run([detection_boxes, detection_scores, detection_classes], \n",
    "                                        feed_dict={image_tensor: image_np})\n",
    "\n",
    "    # Remove unnecessary dimensions\n",
    "    boxes = np.squeeze(boxes)\n",
    "    scores = np.squeeze(scores)\n",
    "    classes = np.squeeze(classes)\n",
    "\n",
    "    confidence_cutoff = 0.45\n",
    "    search_class = 10\n",
    "    # Filter boxes with a confidence score less than `confidence_cutoff`\n",
    "    boxes, scores, classes = filter_boxes1(confidence_cutoff, search_class, boxes, scores, classes)\n",
    "\n",
    "\n",
    "    # The current box coordinates are normalized to a range between 0 and 1.\n",
    "    # This converts the coordinates actual location on the image.\n",
    "    width, height = image.shape[1], image.shape[0]\n",
    "    box_coords = to_image_coords(boxes, height, width)\n",
    "\n",
    "    # Each class with be represented by a differently colored box\n",
    "    #draw_boxes(path, box_coords, classes)\n",
    "\n",
    "    # Crop image to detected traffic light\n",
    "    coords = np.squeeze(box_coords).astype(int)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load the graph and extract the relevant tensors using [`get_tensor_by_name`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name). These tensors reflect the input and outputs of the graph, or least the ones we care about for detecting objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path for the moment\n",
    "model_path = 'C://Users//Silvan//Documents//Udacity_Nanodegree//Project 9_Autonomous Vehicle//Object-Detection-Lab//CarND-Object-Detection-Lab'\n",
    "actual_path = os.getcwd()\n",
    "os.chdir(model_path)\n",
    "\n",
    "#detection_graph = load_graph(SSD_GRAPH_FILE)\n",
    "#detection_graph = load_graph(FASTER_RCNN_GRAPH_FILE)\n",
    "#detection_graph = load_graph(FASTER_RCNN_KITTI_FILE)\n",
    "detection_graph = load_graph(SSDLITE_GRAPH_FILE)\n",
    "\n",
    "# The input placeholder for the image.\n",
    "# `get_tensor_by_name` returns the Tensor with the associated name in the Graph.\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Each box represents a part of the image where a particular object was detected.\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represent how level of confidence for each of the objects.\n",
    "# Score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "# The classification of the object (integer id).\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "# Change path back to former working directory\n",
    "os.chdir(actual_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run detection and classification on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image.\n",
    "\n",
    "image1 = './images/images/frame0140.jpg'\n",
    "image2 = './images/images/frame0116.jpg'\n",
    "image3 = './images/images/frame0001.jpg'\n",
    "image4 = './images/frame0092.jpg'\n",
    "#image4 = './images/image003.jpg' #flowers\n",
    "images = [image1, image2, image3, image4]\n",
    "\n",
    "with tf.Session(graph=detection_graph) as sess:                \n",
    "    # Actual detection.\n",
    "    for path in images:\n",
    "        img = cv2.imread(path)\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect traffic light in image\n",
    "        coords = detect_tl(image, sess)\n",
    "        print(coords)\n",
    "        if coords.size > 0:\n",
    "            print(\"Trueee\")\n",
    "        # Show detected traffic light only\n",
    "        image1 = cv2.resize(image[coords[0]:coords[2], coords[1]:coords[3]], (16,32))\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(image1)\n",
    "        \n",
    "        # Show complete image with annotated boxes\n",
    "        #plt.figure(figsize=(6,4))\n",
    "        #plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKIP!! Make training data for classifier (not necessary to run again!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content = list()\n",
    "error_counter = 0\n",
    "total_counter = 0\n",
    "times = list()\n",
    "base_path = \"images/sign_only\"\n",
    "os.mkdir(base_path)\n",
    "os.mkdir(base_path+\"/images\")\n",
    "os.mkdir(base_path+\"/images2\")\n",
    "with tf.Session(graph=detection_graph) as sess:                \n",
    "    for path, label in content:\n",
    "        img = cv2.imread(path)\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect traffic light in image and record time needed for detection\n",
    "        t0 = time.time()\n",
    "        coords = detect_tl(image, sess)\n",
    "        t1 = time.time()\n",
    "        times.append((t1 - t0) * 1000)\n",
    "        \n",
    "        # Show detected traffic light only\n",
    "        try:\n",
    "            image1 = cv2.resize(image[coords[0]:coords[2], coords[1]:coords[3]], (16,32))\n",
    "\n",
    "            # Saving the image \n",
    "            filename = base_path+path[path.find(\"/\"):path.find(\".jpg\")]+\"_sign_only.jpg\"\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "            cv2.imwrite(filename, image1)\n",
    "            new_content.append([filename, label])\n",
    "        except:\n",
    "            print(\"[Error] Detector could not find a sign in image \", path)\n",
    "            error_counter +=1\n",
    "        total_counter +=1\n",
    "        \n",
    "print(\"TL_Detector did not detect \", error_counter, \" of \", total_counter, \" images -> \",\n",
    "      np.around(100*error_counter/total_counter, decimals=1), \" %\")\n",
    "\n",
    "times = np.array(times)\n",
    "# Save info in seperate file\n",
    "with open(base_path+'/signlabels_sign_only.csv', 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|')\n",
    "    spamwriter.writerows(new_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize needed time for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "plt.title(\"Object Detection Timings\")\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "\n",
    "# Print basic info about times\n",
    "print(\"Mean:\", np.mean(times), \"ms\\nMedian:\", np.median(times), \"ms\\nFirst run:\", times[0],\n",
    "      \"ms\\nFastest run:\", min(times), \"ms\")\n",
    "# Create the boxplot\n",
    "plt.style.use('fivethirtyeight')\n",
    "bp = ax.boxplot(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary for model comparison:\n",
    "\n",
    "Test was made on an old laptop without activated GPU, so run times can not be taken absolutely but must be seen as relative measurement for comparison.\n",
    "\n",
    "---\n",
    "#### Model1: ssd_mobilenet_v1_coco_11_06_2017\n",
    "Did not detect traffic signs on *21.5%* of the training images.\n",
    "Needed *245.86 ms* (median) and *283.43 ms* (mean) for one run.\n",
    "First run needed *6738 ms*.\n",
    "\n",
    "#### Model2: ssdlite_mobilenet_v2_coco_2018_05_09\n",
    "Did not detect traffic signs on *17.1%* of the training images.\n",
    "Needed *152.91 ms* (median) and *176.06 ms* (mean) for one run.\n",
    "First run needed *5640 ms*.\n",
    "\n",
    "#### Model3: faster_rcnn_resnet101_kitti_2018_01_28\n",
    "Needed far longer with 7578 ms (median) per run. So it is about 50 times slower than model2 on this machine which is far to slow and the rate of detection is not relevant as this model is definitely worse than model2.\n",
    "\n",
    "#### Model4: faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017\n",
    "Needed even far longer with 50147 ms (median) per run. So it is about 328 times slower than model2 on this machine which is far to slow and the rate of detection is not relevant as this model is definitely worse than model2.\n",
    "\n",
    "---\n",
    "\n",
    "**It is obvious that model2 seems to be best suited for the traffic light detection task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL_Classifier\n",
    "## Load training data for tl_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 32, 16, 3) uint8\n",
      "(243,) int32\n"
     ]
    }
   ],
   "source": [
    "new_content = list()\n",
    "#filename = 'images/sign_only/signlabels_sign_only.csv'\n",
    "filename = 'images/signlabels_sign_only.csv'\n",
    "with open(filename, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in spamreader:\n",
    "        new_content.append(row)\n",
    "new_content = np.array(new_content)\n",
    "\n",
    "# Load images with signs only for training\n",
    "images, labels = load_content(new_content)\n",
    "\n",
    "print(images.shape, images.dtype)\n",
    "print(labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build tl_Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import adam\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras import backend as K\n",
    "#K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# standard CNN model\n",
    "def build_model():\n",
    "    '''\n",
    "    Build CNN model for light color classification\n",
    "    '''\n",
    "    num_classes = 3\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (3, 3), padding='same',\n",
    "                     input_shape=(32,16,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(16, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='TL_handling//tl_classifier_model.png')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 16, 8)         224       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 5, 32)         4640      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 30,867\n",
      "Trainable params: 30,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#K.set_learning_phase(1)\n",
    "seq_model = build_model()\n",
    "seq_model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Load data and one hot encode\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 194 samples, validate on 49 samples\n",
      "Epoch 1/10\n",
      "194/194 [==============================] - 0s - loss: 3.1354 - acc: 0.7320 - val_loss: 0.0209 - val_acc: 0.9796\n",
      "Epoch 2/10\n",
      "194/194 [==============================] - 0s - loss: 0.0506 - acc: 0.9897 - val_loss: 0.0753 - val_acc: 0.9796\n",
      "Epoch 3/10\n",
      "194/194 [==============================] - 0s - loss: 0.0110 - acc: 0.9948 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 4/10\n",
      "194/194 [==============================] - 0s - loss: 2.3609e-05 - acc: 1.0000 - val_loss: 2.8875e-04 - val_acc: 1.0000\n",
      "Epoch 5/10\n",
      "194/194 [==============================] - 0s - loss: 1.7238e-05 - acc: 1.0000 - val_loss: 1.4844e-04 - val_acc: 1.0000\n",
      "Epoch 6/10\n",
      "194/194 [==============================] - 0s - loss: 1.6855e-04 - acc: 1.0000 - val_loss: 1.2722e-04 - val_acc: 1.0000\n",
      "Epoch 7/10\n",
      "194/194 [==============================] - 0s - loss: 5.0708e-06 - acc: 1.0000 - val_loss: 1.8420e-04 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "194/194 [==============================] - 0s - loss: 1.0367e-06 - acc: 1.0000 - val_loss: 2.2345e-04 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "194/194 [==============================] - 0s - loss: 1.1731e-06 - acc: 1.0000 - val_loss: 2.4257e-04 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "194/194 [==============================] - 0s - loss: 1.2183e-06 - acc: 1.0000 - val_loss: 2.5225e-04 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2208c89d630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1, shuffle=True, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_keras_model(save_model, path):\n",
    "    \"\"\"Saves keras model to given path.\"\"\"\n",
    "    save_model.save_weights(path + 'model.h5')\n",
    "\n",
    "    with open(path + 'model.json', \"w\") as text_file:\n",
    "        text_file.write(save_model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'TL_handling//tl_classifier_'\n",
    "save_keras_model(seq_model, 'TL_handling//tl_classifier_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = model_from_json(open(path + '.json','r').read())\n",
    "    model.load_weights(path + '.h5')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 16, 8)         224       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 5, 32)         4640      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13, 5, 32)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                24640     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 30,867\n",
      "Trainable params: 30,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_path = 'TL_handling//tl_classifier_model'\n",
    "seq_model = load_model(model_path)\n",
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert keras model to tensorflog graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'activation_5_2/Softmax:0' shape=(?, 3) dtype=float32>]\n",
      "[<tf.Tensor 'conv2d_1_input_2:0' shape=(?, 32, 16, 3) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# This line must be executed before loading Keras model.\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "#from keras.models import load_model\n",
    "model = load_model(model_path)\n",
    "print(model.outputs)\n",
    "# [<tf.Tensor 'dense_2/Softmax:0' shape=(?, 10) dtype=float32>]\n",
    "print(model.inputs)\n",
    "# [<tf.Tensor 'conv2d_1_input:0' shape=(?, 28, 28, 1) dtype=float32>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 55 variables.\n",
      "Converted 55 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "#from keras import backend as K\n",
    "#import tensorflow as tf\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TL_handling\\\\tl_classifier_tf_model.pb'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save to ./model/tf_model.pb\n",
    "CLASSIFIER_GRAPH_FILE = 'TL_handling/tl_classifier_tf_model.pb'\n",
    "model_path = 'TL_handling'\n",
    "tf.train.write_graph(frozen_graph, model_path,\"tl_classifier_tf_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Graph model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_graph = load_graph(CLASSIFIER_GRAPH_FILE)\n",
    "# Get output tensor of classifier model\n",
    "output_tensor = classifier_graph.get_tensor_by_name('activation_5_2/Softmax:0')\n",
    "\n",
    "# Get input tensor of classifier model\n",
    "input_tensor = classifier_graph.get_tensor_by_name('conv2d_1_input_2:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on sample\n",
    "## With tf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pred: ', 1, 'True: ', 1, ' [0: red, 1:yellow, 2: green]')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAD8CAYAAACchf2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGTZJREFUeJztnW1wVNd5x//33r27qxXWslpkEcRL\nEJlA5MrGQhowsSuBZDsxKaOQhJrEduNk6k5x0gmMM7apB6VTExtjRhqmMJ7MuJ5MkxiUZqLppM2Q\nAkXUMC6KKYkHCjYYkhjLXiStXlbSvtyXfpB9dyXf5+yye1hZ8Pw+3XOOzr1nr/579jznnOc5im3b\nNhimQNTpbgBzY8BCYqTAQmKkwEJipMBCYqTAQmKkwEJipMBCYqTAQmKk4Cmk8unTp/HKK6/Asiw0\nNzejtbVVVruYGUbeQrIsCy+//DKeeeYZhMNhPP3006ivr8f8+fOF9Tp+fRYAsGn1Yrx64pKT79M1\nso45GnfNVwX9qaIo+ZWZJlnmI5r46Ne/StaZ9+klZNkPdrzoXLfevQRdr1100pZNvw9FoVe1VKKa\nkSKrCN/H3/xFDV0x87k5/ZULFy5cwNy5c1FZWQmPx4PVq1ejp6cn39sxM5y8hTQwMIBwOOykw+Ew\nBgYGpDSKmXnk/dPmtmnArYs8dOgQDh06BAB4/vnnsWn1YgBAeanPuabqOlhWvs28ZgStIMt6jv6S\nrKN7fWTZvKr0MGD2LB9a7878GRS1RABRTbTHI88nTSJvIYXDYfT39zvp/v5+hEKhj/1dS0sLWlpa\nnPRH4yIeI/EYCQCwZMkS9Pb2IhKJwDAMnDhxAvX19fnejpnh5N0jaZqGb33rW9ixYwcsy8KaNWuw\nYMGCrPX8nomvjKIozjUAGOPjZJ0A0RWogt4jZSXJMlXQzfs8gp7Acv9aezwGWcdKDtHPQvozK7An\npW2V/tekTLp70RSi/YLPrOn0z2+uFDSPVFdXh7q6uoIbwcx8eGabkQILiZECC4mRAguJkQILiZFC\nQVZbPvjUiVlqFbZzDQAehZ69/vaDX3PNV1MJso4m+GSpxCjdPg9d0aO4m926SU81jPZGybJHv7rG\nuf6f4/89Ka14dbKeKZj2gOolCmgTv2pRNd3GL75GPyvzsTn9FcNkgYXESIGFxEiBhcRIgYXESKHo\nVltqfAwAYFuWcw0AXsGqopp0t7J00IulZoK26Ep1+vujq3Q923Qv0730NoykQd+vJKMdqmKjRE9/\nnpThvnUGAHSBZZkinucRfOYrb71JluUK90iMFFhIjBRYSIwUWEiMFFhIjBRYSIwUim7+p7WrIFPH\nik0vfOq6+9SAkqJNZL+X/o5ogk3btkJPKQRmzXLNH4vRi8AefwlZ5s8w41VNRWlpqZOOjdH3NA16\ngdvvcV+0jSdidBsF0wm5wj0SIwUWEiMFFhIjBRYSIwUWEiMFFhIjhYLsvscffxx+vx+qqkLTNDz/\n/PNZ6wQCAQCAqqrONQB4DNokt4iVcK9Km+q2TZvIXh9tkg/Hxsiysbh7mSZwr/ZptAt4Mp428S3L\nwlhG2rbpfdnh8nKybHh4xDVf1wQxRyQcR1PwBEJbWxvKysoKbggzs+GfNkYKBfdIO3bsAADce++9\nk+IgMTcXSiHntQ0MDKC8vBxDQ0N49tln8eijj6KmZnJgpqkR2/qHJ8Y7wVIvhkbTyyKKYExw6eJb\n7o0XxWoR7bgUROgyLYHPGHVLQaAqVVBmZ9xw6dJlOH/+XEYh3X5N4LRH+bzZecZlW5FjtJmChJRJ\nZ2cn/H4/1q9fL/y7nxw5DwD4Yv0i/Pq3f3DyPQa9tvTIl5td870KvY0VAofLQEA02KbjGVERCIWD\nbR/tmGhmxHDq/q8TaFyzOl0mcIIMBa99sG2YosE2/cUaH6PX6DLJe4wUj8cx/mFwrHg8jt///vdY\nuHBhvrdjZjh5j5GGhobw4osTMRBN08Tdd9+N5cuXZ60X+/AbY5qmcw0A775zlqzj9bqb0Lrg58s0\n6amBRNL9WwsACcE0RG3t51zzFVBu0uLocPFEuvfzenXMnz/PSb/f+y5ZL5mg268S3aZm+8k6Nugp\nilzJW0iVlZXYtWtXwQ1gbgzY/GekwEJipMBCYqTAQmKkwEJipFD0zf9e74SprKqKcw0At5SVUlUQ\nj7uv/iuCQOm6YJJQEwRlv23Zp+h76u5R1DRbMOkoCK4eLEs7E2iqNjmt0e24+l6ELAMI81+wC8Ew\nCu9PuEdipMBCYqTAQmKkwEJipMBCYqRQdKst8xSozGtLcEqkSpxmZ6fo74EtsFI0jbayVIuOb60S\nBpgqiBxnCU7cMzPq2bYNM5VOl5a4u4cDwKA+SJaNjrvvK7ctesuNRR3NdQ1wj8RIgYXESIGFxEiB\nhcRIgYXESIGFxEih6Oa/40mjTD4Ky7JoE5pacFQs2jMikRIs6Pro/csjg7QXSXBBlWu+wDt88hzH\nFBLJDFPdtmEk014lQ0MCbxaBx5RBeJ946FkNQCnckYh7JEYKLCRGCiwkRgosJEYKLCRGCiwkRgpZ\nzf99+/bh1KlTCAaD2L17NwAgFouhvb0dV69eRUVFBbZs2YJZRDDzqegfBkRXYDvXADA+JgreQETY\nELVe4CudAr0S3ts/QJbNXXira76H2J0AADDpuQHbStezp6QjffQKvygwhaK59w0m6DkDVS9CwPam\npiZs27ZtUl5XVxdqa2uxZ88e1NbWoqurq+CGMDObrEKqqan5WG/T09ODxsZGAEBjYyN6enquT+uY\nGUNeY6ShoSGEQiEAQCgUwvDwsNRGMTOP675EMjVi2331SwEAwVK/cw0Af14zn7zHl5tWuuYrhA9X\nNhRBFDVLsN5BBehSbEFUNkEcs8xnVVcvwU/3/8JJJwRn8oojzlHtp9uoCO6XK3kJKRgMIhqNIhQK\nIRqNCqPatrS0TIot+ZvfTkRsu69+qXMNAGdOnyTvsWv7k675uiBim2jrrs9HxzMaj9OR4+688w7X\nfI9ND7ZTKXqrbabj50/3/wLfePArTvrCpXfIeqLB9liCODFKFWwh9tL3G+2jjaBJ98jpr6ZQX1+P\n7u5uAEB3dzcaGhryuQ1zA5E1hmRHRwfOnj2LkZERBINBbNy4EQ0NDWhvb0dfXx/mzJmDrVu35mz+\nVy6biOp28F9/ivu/+g0nf3aAXpH/w7nfueZrApPWEOwmmF1+C1k2EqPHez7CTFZMQeBTRThH4Vwd\nOvIaWtbe7aRTgiX+gOBdDQy6t1/V6V5YNEBIDAvidGaQ9afte9/7nmv+9u3bc3oAc3PAM9uMFFhI\njBRYSIwUWEiMFFhIjBSKvvn/ka98EQAQDgWdawD44Mp7ZJ0P3nrTNd8SBGX3+Glz1+enJ+e8+myy\nDMR5KWaCPmreq9HtyDx6QlMVBEvTZr2t0jPRhkmb5BoxE2EJnCEC/tymbkRwj8RIgYXESIGFxEiB\nhcRIgYXESIGFxEih6Ob/v/3iAADgrzZ93bkGxO7nCmHmi4KQlwWDZJkqOOY9adCb7qllci+9GI9k\nnDbVM1++DRMpK+qkNUHMAF2ndy/oHqJvUAWr/zb9PnKFeyRGCiwkRgosJEYKLCRGCiwkRgpFt9re\nuXAJAJCIJ5xrACjx0laF1+u+yJpK0YulIu+NsViULJtFG0R4vecF1/zyIG3p2QJXpc5XTzvX86qC\n+Idn1znpH7T9O1kvFRd4piTcrT1vgG5HbMw9yPu1wD0SIwUWEiMFFhIjBRYSIwUWEiMFFhIjhbwi\ntnV2duLw4cNO8IhNmzahrq4upwcaysQKp60ozjUAmCodyCCRdA+MoAqiioj2SnvoA73x6oEtZFlw\n9vuu+ZbyNlnHStEO0Ru+VuNclwZKJqV/tp82/98+Q5v/uu7+Hk2BY7ZXsL89V7IKqampCV/4whew\nd+/eSfnr1q3D+vXrC24Ac2OQV8Q2hplK3jPbBw8exLFjx1BdXY1HHnmExXaTkzWsDQBEIhHs3LnT\nGSMNDg4646MDBw4gGo1i8+bNrnWnRmz77RtvAAA+t2wZ/u/cOefvVEXQORJR1ARDJGge0XeEHmNU\nV1eSZSUBqoQIbgVMhKsli9IR4DT1VphWxEm/c9F9PAYA8XGRzxtRJnhXIgmsqFtBV8wgrx5p9uy0\nE2FzczN27txJ/u3UiG0NqyZiAPW8/ppzDQABQRQy1aQG2/QLCIVDZJmlfECWvfpzerB9+3L3/4aS\n52DbtjIH23+H0bE9Tvrx7+wi6719hh4c90Xdyyydbocp2CGZa3ykvMz/aDS96Hny5EksWLAgn9sw\nNxB5RWw7c+YMLl++DEVRUFFRgccee8yJcpuNn/16IpTy/atrcPDEWSf/g3fpmInPPPG3rvlqMkbW\nCQriWqaMfrLs4h/+kSyzPadd8wP+q2Qdkzg/DQASqXnpe3ifw1jy6YyKd5L1bl+2jSzr63Pv2S2N\n3lhuCYKRjg/TAewzySti29q1a3O6OXPzwDPbjBRYSIwUWEiMFFhIjBRYSIwUir753/Z+uJSiqOlr\nAEmbjqKWMNxnKLyi6VrBlLKo1uV3esmy6qXuUwpWapyskzToCT1vhuu1omjwetLp/T//JVlPdPS6\nDffJRdH5JYomOss9N7hHYqTAQmKkwEJipMBCYqTAQmKkwEJipFB08x/WR5vK7IxrQNfpCGWa5r6i\nrSr0BjXBYjcG++iy5pZ9ZNmFC//k/izvFbKOmaLN/+R4ehNdWZkfo8Ppo1mfefqfyXpE3PiJ5xHn\nvKk2vfnOSHLENuYTAguJkQILiZECC4mRAguJkULRrTZNm1gyVTKuAcA0aQvMMNwjs3k02jNiZHSE\nLJsdFiwQj9PtWLToO675q1aSVXDxIl0Wy2jifxxciwfuf9JJi7yzPIITs71+dwssIYhgp2qF9yfc\nIzFSYCExUmAhMVJgITFSYCExUmAhMVLIav739fVh7969GBwchKIoaGlpwQMPPIBYLIb29nZcvXoV\nFRUV2LJlS06hbRTn+Cgl41oMFXxNE5ituiAohUcYHJ5ewNSJ6Yb3e+nPMRobIsvKwxXpNmmeSenY\nCL0P3KOTYVGQSERc8zWdnvLInIbJl6xC0jQNDz/8MKqrqzE+Po6nnnoKt99+O44ePYra2lq0trai\nq6sLXV1deOihhwpuEDMzyfrTFgqFUF1dDQAoKSlBVVUVBgYG0NPTg8bGRgBAY2Mjenp6rm9LmU80\n1zRGikQiuHTpEj7zmc9gaGjIiUASCoUwPDx8XRrIzAxyXiKJx+PYvXs3vvnNbyIQoH+jpzI1Ytu9\nKxYDAMpKfc41ACT/bC55j7UrDrvmK4JAW6rgZ1/o4yXyhyOeJ4qumzLoXWgeLf36q5d8FvsP/MZJ\nmxa9/KMI1k+SxCY1RRTeTlSWIzkJyTAM7N69G/fccw9WrpxYWAoGg4hGowiFQohGo04owKlMjdj2\nn29MnIh074rFzjUA/PHi2Y/V/Yi/f+Ix13y/Rp+OVOKnX05JCT3YVgkHQ4AebJf66cF2JEIPtkPl\n6cH1/gO/wYN/eZ+Tznew/acr1GCbNj40gYPk8CC9ZplJ1p8227bx0ksvoaqqCl/60pec/Pr6enR3\ndwMAuru70dDQkNMDmRuTrD3S+fPncezYMSxcuBDf//73AUwEaG9tbUV7ezuOHDmCOXPmYOvWrQU1\nRKNOhwbd8xqGYK+xKAC84OcmMUbvsfbp7hvBR0QHc1v0ad/Dg+nPbJrKpHTKEkRYE3z/qZ0BosB8\norPtciWrkJYtW4bOzk7Xsu3btxfcAObGgGe2GSmwkBgpsJAYKbCQGCmwkBgpTNvmfyiTV51F5inl\nUqwLWm8JZoaTAnNXsenvVonf/aC30C2zXfMBYDRGH4Xe+0H6BAXTtDA4nHarFs1Ee330tAcVIF5V\n6ElHXeBMkCvcIzFSYCExUmAhMVJgITFSYCExUmAhMVIouvmfaZZPMtFt2tzVNPf9Porge6AIphPG\nY/QK//wq+hDD8tlh13yfRr9Gn5dexa/81CLnuiQQwPLly530G/97iqwX8Ij2Fl37uxLuosgR7pEY\nKbCQGCmwkBgpsJAYKbCQGCkU/5gtVXO/zsMjRnSCtQLastFVegFz9ix6AVYn3IBsk7Z6PAK38pSR\n9hSxbWtSet68W8l6g1E6UDj1TjwCy1LgnZUz3CMxUmAhMVJgITFSYCExUmAhMVJgITFSyDtiW2dn\nJw4fPuwEj9i0aRPq6uqyPjAdCUSZFBXE46Gbkkq6B4vQdXrOwCJO5gYATaGf5RHYwrbpvg9cEQSe\nELmHG3bG9IdtIZFI7+9WVHrPuQgq0oppC4JjiI7tzpG8I7YBwLp167B+/fqCG8HMfLIKKRQKOQG1\nMiO2MUwmeUdsA4CDBw/iiSeewL59+xCLxa5LA5mZgWKLHMoyiMfjaGtrw4YNG7By5UoMDg4646MD\nBw4gGo1i8+bNH6s3NWLbwMiE71ZZwIvhsfTYJ5Wkj8r80+V3XPNVUcQ20VhH8JH9gmi4E0fxuOXS\n97OFZen7LV5cjUuX0p9T5Jcn2ogmqkch8qG7887s414gRyEZhoGdO3fijjvumBRs6yMikQh27tyJ\n3bt3Z33gz197CwDQfMdCHP7dH538K5fOkXW2/PUm1/yATg9kAyX0zsRUgo70tuyznyXLVEJIoihv\nKeKMWWDyYPtffrIfDz/0oJOOjY2S9aL99NBidNzdIdMm2g4AuiB08lA0t1+avCO2RaNpL9GTJ09i\nwQJ6iypz45N3xLbjx4/j8uXLUBQFFRUVeOwx9ziPU8ncO5x5LeoYVWofsiLq/uleR1HpZ12N0Cvr\nt1bMcX+WST9LFUSi82WcLK4ogC8jFuXFS71kvZKAYM92ivhsgimPlFX4nu28I7blMmfE3DzwzDYj\nBRYSIwUWEiMFFhIjBRYSI4Xib/7PMPNznFQnZ14tm57s8/nco6t9+GCy6L333iPLYsQR8MEyevLT\nEJjWgxnR3OKJOM69dd5JKwKnAZHTQzLl/jx/oISsg2ThAdu5R2KkwEJipMBCYqTAQmKkwEJipMBC\nYqRQfPNfcb8W7YmhzN2UTZutos1fomeVhWjff2r/VyxOn/Yoimng86dPglQVbVI6aQiOIhV8/VUi\nrkE8Tr+rXKdhRHCPxEiBhcRIgYXESIGFxEiBhcRIgYXESKH4AdudFXt70up9IkEHUfd43U1anx1w\nzQeAsXF6Q741SvvQqarolRC+/wI//YRgGkLV0p/LME30D6V3Fxgp+p4ewhkCALxed8eAxBht/oum\nQ3KFeyRGCiwkRgosJEYKLCRGCiwkRgpZrbZkMom2tjYYhgHTNLFq1Sps3LgRkUgEHR0diMViWLx4\nMb773e8Ko659hN83YXEoquJcA4CqCqKvme56T9m09QLQZSKXbYGxBI9OREOzaAsRAgtL8WRYnYoK\nxTMrnRZEWFMV+p5xwjrTPbSFa1uFL9pm/c/ruo62tjb4/X4YhoHt27dj+fLl+NWvfoV169bh85//\nPH70ox/hyJEjuO+++wpuEDMzyfrTpigK/P4JLwnTNGGaJhRFwZkzZ7Bq1SoAQFNTE3p6eq5vS5lP\nNDlNSFqWhSeffBLvv/8+7r//flRWViIQCDinFZaXl3M4wJucnISkqip27dqF0dFRvPjii7hy5UrO\nD5gasa3xtok4Srf4vc41ANxV7R4yBgDuOXbYNV8hZpqvG9TONptuh2j0oWTsUFu2dClOHEl/TtFm\nM0UQNIuqJ4rKJoNrWiIpLS1FTU0N3n77bYyNjcE0TWiahoGBAZSXl7vWaWlpQUtLi5PuPvMnAEDj\nbQucawC4dO4M+dwnNn/bvfGCHZIiFEHIQCIC8sTziMG2JRhsmwIpaRkD4BNHDmP12mYnnSIcHQHA\nKxhsG4Szo8fjJeuIRJsYi5JlmWQdIw0PD2N0dCIMXTKZxJtvvomqqircdttteP311wEAR48eRX19\nfU4PZG5MsvZI0WgUe/fuhWVZsG0bd911F1asWIH58+ejo6MD+/fvx+LFi7F27dqcHjg+PiFKy7Kc\nawC49Vb6fDJdJyKUJQVR3jz5deWqV+QqTfQSgrPQPIJFYNOYvIHdMtNpr06b65YgCLyvxH0BVrXp\nhdkf/vCHZFmuZBXSokWL8MILL3wsv7KyEs8991zBDWBuDHhmm5ECC4mRAguJkQILiZECC4mRQs5n\nkTCMiGnrkZ566qnpevQnkpn+PvinjZECC4mRwrQJKXMhl5n574MH24wU+KeNkULRXbYB4PTp03jl\nlVdgWRaam5vR2to6Hc2YNvbt24dTp04hGAw6p27GYjG0t7fj6tWrqKiowJYtWzBr1qwsd/rkUPQe\nybIsvPzyy9i2bRva29tx/PhxvPvuu8VuxrTS1NSEbdu2Tcrr6upCbW0t9uzZg9raWnR1dU1T6/Kj\n6EK6cOEC5s6di8rKSng8Hqxevfqmcxyoqan5WG/T09ODxsZGAEBjY+OMeydFF9LAwADC4bCTDofD\n7DgAYGhoCKFQCAAQCoUwPDw8zS26NoouJDcj8XpvTGeuP0UXUjgcRn9/v5Pu7+93vok3M8Fg0Dm5\nPBqNoqysbJpbdG0UXUhLlixBb28vIpEIDMPAiRMn2HEAQH19Pbq7uwEA3d3daGhomOYWXRvTMiF5\n6tQp/PjHP4ZlWVizZg02bNhQ7CZMKx0dHTh79ixGRkYQDAaxceNGNDQ0oL29HX19fZgzZw62bt06\no8x/ntlmpMAz24wUWEiMFFhIjBRYSIwUWEiMFFhIjBRYSIwUWEiMFP4fz2bDDsFB9bgAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2208e61f438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "take_ID = True # Visu image from dataset\n",
    "times = list()\n",
    "time_labels = list()\n",
    "\n",
    "if take_ID:\n",
    "    # Visualize Image from dataset\n",
    "    ID = 80 #yellow\n",
    "    #ID = 40 #red\n",
    "    #ID = 150 #green\n",
    "    img = images[ID]\n",
    "    label = labels[ID]\n",
    "    plt.imshow(img)\n",
    "else:\n",
    "    # Or load something else\n",
    "    time_labels.append('start absolute')\n",
    "    times.append(time.time())   ######## time1: start absolute\n",
    "    label = \"?\"\n",
    "    path = \"images/image002.jpg\" #001, 002, 009\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    # Detect traffic light in image\n",
    "    time_labels.append('start of detection')\n",
    "    times.append(time.time())   ######## time2: start of detection\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        coords = detect_tl(img, sess)\n",
    "    time_labels.append('end of detection')\n",
    "    times.append(time.time())   ######## time3: end of detection\n",
    "    # Show detected traffic light only\n",
    "    img = cv2.resize(img[coords[0]:coords[2], coords[1]:coords[3]], (16,32))\n",
    "    #plt.figure(figsize=(2, 2))\n",
    "    #plt.imshow(img)\n",
    "\n",
    "image_np = np.expand_dims(np.asarray(img, dtype=np.uint8), 0)\n",
    "time_labels.append('end of preparation for classification')\n",
    "times.append(time.time())   ######## time4: end of preparation for classification\n",
    "#pred_label = np.argmax(seq_model.predict(image_np)[0])\n",
    "with tf.Session(graph=classifier_graph) as sess1:\n",
    "    predictions = sess1.run(output_tensor, {input_tensor: image_np})\n",
    "    pred_label = np.argmax(predictions[0])\n",
    "#time_labels.append('end of classification')\n",
    "times.append(time.time())   ######## time5: end of classification\n",
    "\"Pred: \",pred_label, \"True: \", label, \" [0: red, 1:yellow, 2: green]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pred: ', 1, 'True: ', 1, ' [0: red, 1:yellow, 2: green]')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAD8CAYAAACchf2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2208e3c95c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "take_ID = True # Visu image from dataset\n",
    "times = list()\n",
    "time_labels = list()\n",
    "\n",
    "if take_ID:\n",
    "    # Visualize Image from dataset\n",
    "    ID = 80 #yellow\n",
    "    #ID = 40 #red\n",
    "    #ID = 150 #green\n",
    "    img = images[ID]\n",
    "    label = labels[ID]\n",
    "    plt.imshow(img)\n",
    "else:\n",
    "    # Or load something else\n",
    "    time_labels.append('start absolute')\n",
    "    times.append(time.time())   ######## time1: start absolute\n",
    "    label = \"?\"\n",
    "    path = \"images/image002.jpg\" #001, 002, 009\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img)\n",
    "    # Detect traffic light in image\n",
    "    time_labels.append('start of detection')\n",
    "    times.append(time.time())   ######## time2: start of detection\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        coords = detect_tl(img, sess)\n",
    "    time_labels.append('end of detection')\n",
    "    times.append(time.time())   ######## time3: end of detection\n",
    "    # Show detected traffic light only\n",
    "    img = cv2.resize(img[coords[0]:coords[2], coords[1]:coords[3]], (16,32))\n",
    "    #plt.figure(figsize=(2, 2))\n",
    "    #plt.imshow(img)\n",
    "\n",
    "\n",
    "image_np = np.expand_dims(np.asarray(img, dtype=np.uint8), 0)\n",
    "time_labels.append('end of preparation for classification')\n",
    "times.append(time.time())   ######## time4: end of preparation for classification\n",
    "pred_label = np.argmax(seq_model.predict(image_np)[0])\n",
    "time_labels.append('end of classification')\n",
    "times.append(time.time())   ######## time5: end of classification\n",
    "\"Pred: \",pred_label, \"True: \", label, \" [0: red, 1:yellow, 2: green]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on all images in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset:  (293, 600, 800, 3) uint8\n",
      "Shape of labels:  (293,) int32\n",
      "TL_Detector did not detect  0  of  10  images ->  0.0  %\n",
      "TL_Classifier missclassified  0  of  10  images ->  0.0  %\n"
     ]
    }
   ],
   "source": [
    "# Init variables\n",
    "error_counter = 0\n",
    "total_counter = 0\n",
    "miss_counter = 0\n",
    "label_names = [\"red\", \"yellow\", \"green\"]\n",
    "delta_times_total = list()\n",
    "\n",
    "# Load full images\n",
    "images, labels, paths = load_content(content, verbose=2)\n",
    "print(\"Shape of dataset: \", images.shape, images.dtype)\n",
    "print(\"Shape of labels: \", labels.shape, labels.dtype)\n",
    "\n",
    "#with tf.Session(graph=detection_graph) as sess:\n",
    "sess = tf.Session(graph=detection_graph)\n",
    "sess1 = tf.Session(graph=classifier_graph)\n",
    "for ID in range(len(images[:10])):\n",
    "    delta_times = list()\n",
    "    t0 = time.time()\n",
    "    image = images[ID]\n",
    "    label = labels[ID]\n",
    "\n",
    "    # Try to detect traffic light\n",
    "    coords = detect_tl(image, sess)\n",
    "\n",
    "    # Save needed time\n",
    "    t1 = time.time()\n",
    "    dt = t1-t0\n",
    "    delta_times.append(dt) #Detection\n",
    "    try:\n",
    "        # Show detected traffic light only\n",
    "        img = cv2.resize(image[coords[0]:coords[2], coords[1]:coords[3]], (16,32))\n",
    "        image_np = np.expand_dims(np.asarray(img, dtype=np.uint8), 0)\n",
    "         # Save needed time\n",
    "        t2 = time.time()\n",
    "        dt = t2-t1\n",
    "        delta_times.append(dt) #Preparation\n",
    "        pred_label = 4\n",
    "        \"\"\"with tf.Graph().as_default():\n",
    "            with tf.Session() as sess1:\n",
    "                K.set_session(sess1)                   \n",
    "                seq_model = load_model(model_path)\n",
    "                pred_label = np.argmax(seq_model.predict(image_np)[0])\"\"\"\n",
    "        #with tf.Session(graph=classifier_graph) as sess1:\n",
    "        predictions = sess1.run(output_tensor, {input_tensor: image_np})\n",
    "        pred_label = np.argmax(predictions[0])\n",
    "        # Save needed time\n",
    "        t3 = time.time()\n",
    "        dt = t3-t2\n",
    "        delta_times.append(dt) # Prep Classification\n",
    "\n",
    "        if pred_label != label:\n",
    "            miss_counter += 1\n",
    "            print(\"[ERROR] Classifier misclassified image \", paths[ID], \" with \", label_names[label], \" (true) as \", label_names[pred_label], \" (pred)\")\n",
    "\n",
    "    except:\n",
    "        print(\"[Error] Detector could not find a sign in image \", paths[ID])\n",
    "        error_counter +=1\n",
    "    total_counter +=1\n",
    "    delta_times_total.append(delta_times)\n",
    "sess.close()\n",
    "sess1.close()\n",
    "print(\"TL_Detector did not detect \", error_counter, \" of \", total_counter, \" images -> \",\n",
    "  np.around(100*error_counter/total_counter, decimals=1), \" %\")\n",
    "print(\"TL_Classifier missclassified \", miss_counter, \" of \", total_counter-error_counter, \" images -> \",\n",
    "  np.around(100*miss_counter/(total_counter-error_counter), decimals=1), \" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = delta_times_total\n",
    "b = np.zeros([len(a),len(max(a,key = lambda x: len(x)))])\n",
    "for i,j in enumerate(a):\n",
    "    b[i][0:len(j)] = j\n",
    "time_deltas = np.hstack((b,b.sum(axis=1).reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26984596, 0.        , 0.00199807, 0.2713443 ])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGHCAYAAACu1mg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XtYFeX+9/HPggUioggs0UAtJUyt\nTJPUMncGZGZW1i8PHbzUtPLCJH4pSbnbPtlOJaUMz7ktqu2vNCvbdtgZ9TPPB0JLUfOUu9yoyEFF\nCTmsef7wcT2xQVgWLBjn/bouL1kz95r5jjcMH+97Zo3NMAxDAAAAJuJV3wUAAABcKgIMAAAwHQIM\nAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMgDrTt29fjRkzpsFsB8Dlw8bnwAC4VDabrdr1V155pQ4f\nPqz8/HzZ7XY1a9bsD+2vtrYD4PJBgAFwyY4dO+b6euvWrbrvvvu0detWtWnTRpLk7e2tFi1a1Fd5\nACyAKSQAl6xVq1auP8HBwZKkFi1auJZdCC//OfXTt29fjR49Wn/+858VGhqq5s2ba/LkyXI6nZo6\ndapatmypFi1aaPLkyRX2V9V2xowZo5deeslVw8iRI3X27FlXG6fTqeeff14tWrRQQECAhg0bptmz\nZ8tut7vaHDlyRP/1X/8lh8Ohxo0bq3379po5c2ad/JsBqF32mpsAQO1ZsWKFxo4dq/Xr12v9+vUa\nPXq0tm/fruuuu07r1q3Tpk2bNHLkSN1666266667qt3OqFGjtGbNGh0+fFjDhg3TlVdeqRdffFGS\nNHv2bKWmpmrBggXq1auXVq1apalTp1bYRlxcnIqKipSenq7mzZvrp59+qjC6BKDhIsAA8Kh27dop\nOTlZktShQwelpKTol19+0eeff+5a9uqrr+rrr7+uNsC0bdtWr732miSpY8eOGjZsmFavXu0KMCkp\nKfrv//5vDR8+XJL0zDPPaOvWrVqxYoVrG//61790//33q2vXrpKkq666qtaPF0DdYAoJgEfdcMMN\nFV63atVKXbp0qbQsJyen2u1cCB0XhIeH6/jx45Kk06dPKzs7W7169arQ5uabb67wOiEhQdOmTVPP\nnj01adIkrV279pKOBUD9IcAA8CgfH58Kr202W5XLnE5ntdvx9fW96Hsu3JtQ091So0aN0r/+9S+N\nHTtWR48e1V133aVHH33UreMAUL8IMAAuO4GBgQoLC9OmTZsqLN+8eXOltldccYVGjRqld955R0uW\nLNHSpUt1+vRpT5UK4HfiGhgAl6UJEyZoypQp6tixo3r06KHPPvtMq1evrjAq89RTT2nAgAG65ppr\nVFxcrI8++kht2rRR06ZN67FyAO4gwAC4LCUkJOjEiRN6+umnVVxcrIEDB2rChAmaPn26q41hGEpI\nSNAvv/wif39/9erVS1988UWNU08A6h8fZAfAMh577DF9//33+u677+q7FAB/ECMwAC5L2dnZ+vjj\nj3X77bfL29tbq1at0jvvvKO5c+fWd2kAagEjMAAuS8ePH9fQoUP1ww8/qLi4WFdffbXGjx+vxx9/\nvL5LA1ALCDAAAMB0uI0aAACYDgEGAACYDgEGAACYjunvQsrOzq7vEuqFw+FQbm5ufZcBD6PfrYc+\ntyar9ntYWJjbbRmBAQAApkOAAQAApkOAAQAApkOAAQAApkOAAQAApkOAAQAApkOAAQAApkOAAQAA\npkOAAQAApkOAAQCggVi5cqWio6PVuHFjRUdHa+XKlfVdUoPl0UcJOJ1OJSUlKTg4WElJSRXWlZaW\nau7cuTp06JCaNm2qhIQEhYaGerI8AADqzcqVK5WcnKxZs2ZpwIAB+vzzzzVx4kRJ0qBBg+q5uobH\noyMwn3/+ucLDw6tc980336hJkyaaM2eO7r77bi1dutSTpQEAUK9SU1M1a9Ys9e7dWz4+Purdu7dm\nzZql1NTU+i6tQfJYgMnLy1NmZqZiYmKqXJ+RkaG+fftKknr16qVdu3bJMAxPlQcAQL3av3+/evTo\nUWFZjx49tH///nqqqGHzWIBJS0vTo48+KpvNVuX6/Px8hYSESJK8vb3l7++vwsJCT5UHAEC9ioyM\n1NatWyss27p1qyIjI+upoobNI9fAfPfddwoMDFT79u2VlZVVZZuqRluqCjvp6elKT0+XJM2YMUMO\nh6N2izUJu91u2WO3Mvrdeuhz65g8ebKeffZZLVq0SLfddpt27dqlZ599VlOnTuV7oAoeCTA//vij\nMjIytH37dpWUlOjXX39Vamqq4uPjXW1CQkKUl5enkJAQlZeXq6ioSAEBAZW2FRsbq9jYWNfr3Nxc\nTxxCg+NwOCx77FZGv1sPfW4dMTExKiwsVHx8vPbv36/IyEglJiYqJibGMt8DYWFhbre1GR6+0CQr\nK0urVq2qdBfSP//5T/3888964okntGHDBm3ZskXPPPNMjdvLzs6uq1IbpJUrVyo1NdX1zR0fH8/V\n6RbCLzProc+tyar9fikBpl4/B2bZsmXKyMiQJEVHR+vMmTMaP368Pv30Uz3yyCP1WVqDdOEWu5de\nekmnT5/WSy+9pOTkZD4nAABgOR4fgaltVhqBiY6O1ksvvaTevXu70vmGDRv0wgsv6Jtvvqnv8uAB\nVv1fmZXR59Zk1X43zQgMLg232AEAcB4BxkS4xQ4AgPMIMCYSHx+viRMnasOGDSotLdWGDRs0ceLE\nCndzAQBgBR59FhL+mAt3G73wwgsaNmyYIiMjNWnSJO5CAgBYDgHGZAYNGqRBgwZZ9gIvAAAkppAA\nAIAJEWAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDp\nEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAA\nAIDpEGAAAIDpEGAAAIDpEGAAAIDp2D2xk5KSEk2ZMkVlZWUqLy9Xr169NGTIkApt1qxZo3fffVfB\nwcGSpP79+ysmJsYT5QEAAJPxSIDx8fHRlClT5Ofnp7KyMv3lL39R165d1aFDhwrtbrnlFo0ePdoT\nJQEAABPzyBSSzWaTn5+fJKm8vFzl5eWy2Wye2DUAALgMeWQERpKcTqcmTZqkY8eO6c4771RkZGSl\nNlu2bNGePXt0xRVXaMSIEXI4HJ4qDwAAmIjNMAzDkzs8e/asZs2apVGjRqlt27au5YWFhfLz85OP\nj49Wr16tTZs2acqUKZXen56ervT0dEnSjBkzVFJS4rHaGxK73a6ysrL6LgMeRr9bD31uTVbtd19f\nX7fbejzASNIHH3ygRo0a6d57761yvdPp1KhRo/T222/XuK3s7OzaLs8UHA6HcnNz67sMeBj9bj30\nuTVZtd/DwsLcbuuRa2BOnz6ts2fPSjp/R9LOnTsVHh5eoU1BQYHr64yMDLVu3doTpQEAABPyyDUw\nBQUFmjdvnpxOpwzD0M0336zu3btr2bJlioiIUFRUlL744gtlZGTI29tbAQEBiouL80RpAADAhOpl\nCqk2MYUEK6HfrYc+tyar9nuDm0ICAACoTQQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQY\nAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABg\nOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQYAABgOgQY\nAABgOgQYAABgOgQYAABgOnZP7KSkpERTpkxRWVmZysvL1atXLw0ZMqRCm9LSUs2dO1eHDh1S06ZN\nlZCQoNDQUE+UBwAATMYjIzA+Pj6aMmWKZs6cqVdeeUU7duzQvn37KrT55ptv1KRJE82ZM0d33323\nli5d6onSAACACXkkwNhsNvn5+UmSysvLVV5eLpvNVqFNRkaG+vbtK0nq1auXdu3aJcMwPFEeAAAw\nGY9MIUmS0+nUpEmTdOzYMd15552KjIyssD4/P18hISGSJG9vb/n7+6uwsFDNmjWr0C49PV3p6emS\npBkzZsjhcHjmABoYu91u2WO3Mvrdeuhza6Lfa+axAOPl5aWZM2fq7NmzmjVrln7++We1bdvWtb6q\n0Zb/HKWRpNjYWMXGxrpe5+bm1k3BDZzD4bDssVsZ/W499Lk1WbXfw8LC3G7r8buQmjRpos6dO2vH\njh0VloeEhCgvL0/S+WmmoqIiBQQEeLo8AABgAh4JMKdPn9bZs2clnb8jaefOnQoPD6/Qpnv37lqz\nZo0kafPmzbr22murHIEBAADwyBRSQUGB5s2bJ6fTKcMwdPPNN6t79+5atmyZIiIiFBUVpejoaM2d\nO1fjx49XQECAEhISPFEaAAAwIZth8lt9srOz67uEemHV+VGro9+thz63Jqv2e4O+BgYAAOCPIsAA\nAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADT\nIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAAAADTIcAA\nAADTsVe3ctmyZW5txNvbWw8++GCtFAQAAFCTagPMypUr1adPnxo3snnzZgIMAADwmGoDjI+Pj+Li\n4mrcyLZt22qtIAAAgJpUew3Mm2++6dZGFi9eXCvFAAAAuKPaAGO3X3yApqSkRGVlZTW2AwAAqG1u\n34X0zjvv6MCBA5KkzMxMjRo1SiNHjlRGRkadFQcAAFAVtwPM+vXr1aZNG0nSihUrNH78eD377LN6\n77336qw4AACAqrg993Pu3Dk1atRIhYWFOn78uHr16iVJys3NrfG9ubm5mjdvnk6ePCmbzabY2FgN\nGDCgQpusrCy98sorCg0NlST17NmTO5sAAECV3A4wYWFhWrdunY4dO6YuXbpIkk6fPi1fX98a3+vt\n7a3hw4erffv2+vXXX5WUlKQuXbqodevWFdp16tRJSUlJl3gIAADAatyeQho9erS+/PJLZWVlaejQ\noZKk77//3hVmqhMUFKT27dtLkho3bqzw8HDl5+f/zpIBAIDV2QzDMDy5w5ycHE2ZMkUpKSny9/d3\nLc/KylJKSopCQkIUFBSk4cOHu665+a309HSlp6dLkmbMmKGSkhKP1d6Q2O12111gsA763Xroc2uy\nar+7M6tzQbUBJjs7W2FhYTVuxN12xcXFmjJlih544AH17NmzwrqioiJ5eXnJz89PmZmZSktLU2pq\nqlv7tiKHw+HW9Ue4vNDv1kOfW5NV+92dLHFBtVNIzz33nFsbmTx5co1tysrKlJKSoj59+lQKL5Lk\n7+8vPz8/SdKNN96o8vJynT592q39AwAAa6n2It5z585pypQpNW6kpmEuwzC0cOFChYeHa+DAgVW2\nOXnypAIDA2Wz2XTgwAE5nU41bdq0xn0DAADrqTbAjB071q2NxMTEVLv+xx9/1Nq1a9W2bVslJiZK\nkh566CHX8Fi/fv20efNmrV69Wt7e3vL19VVCQoJsNptb+wcAANbi8Yt4axvXwMBK6Hfroc+tyar9\nXmvXwAAAADREBBgAAGA6BBgAAGA6BBgAAGA61d6FNGfOHLfuBHrqqadqrSAAAICaVDsC06pVK7Vs\n2VItW7aUv7+/tm3bJqfTqeDgYDmdTm3btq3C4wAAAAA8odoRmMGDB7u+fvnll5WUlKROnTq5lu3d\nu1cffvhh3VUHAABQBbevgdm3b58iIyMrLLv66qu1b9++Wi8KAACgOm4HmHbt2um9995zPf25pKRE\n77//vq666qq6qg0AAKBK1U4h/VZcXJxSU1M1YsQIBQQE6MyZM4qIiFB8fHxd1gcAAFCJ2wEmNDRU\nf/3rX5Wbm6uCggIFBQXJ4XDUZW0AAABVuqTPgSksLNTu3bu1e/duORwO5efnKy8vr65qAwAAqJLb\nAWb37t1KSEjQunXrXHceHTt2TIsXL66z4gAAAKridoBJS0tTQkKCJk+eLG9vb0nn70I6ePBgnRUH\nAABQFbcDzIkTJ3T99ddXWGa321VeXl7rRQEAAFTH7QDTunVr7dixo8KynTt3qm3btrVeFAAAQHXc\nvgtp+PDhSk5OVrdu3VRSUqI33nhD3333nRITE+uyPgAAgErcDjAdOnTQzJkztW7dOvn5+cnhcGja\ntGkKCQmpy/oAAAAqcTvASFJwcLDuu+++uqoFAADALdUGmDlz5shms9W4kaeeeqrWCgIAAKhJtRfx\ntmrVSi1btlTLli3l7++vbdu2yel0Kjg4WE6nU9u2bZO/v7+nagUAAJBUwwjM4MGDXV+//PLLSkpK\nUqdOnVzL9u7d6/pQOwAAAE9x+zbqffv2KTIyssKyq6++Wvv27av1ogAAAKrjdoBp166d3nvvPZWU\nlEiSSkpK9P777+uqq66qq9oAAACq5PZdSHFxcUpNTdWIESMUEBCgM2fOKCIiQvHx8XVZHwAAQCVu\nB5jQ0FD99a9/VW5urgoKChQUFCSHw1GXtQEAAFTJ7SkkSSosLNTu3bu1e/duORwO5efnKy8vr65q\nAwAAqJLbAWb37t1KSEjQunXrXHceHTt2TIsXL66z4gAAAKri9hRSWlqaEhISdP3112vUqFGSzt+F\ndPDgwRrfm5ubq3nz5unkyZOy2WyKjY3VgAEDKrQxDENvvfWWtm/frkaNGikuLk7t27e/xMMBAABW\n4HaAOXHihK6//vqKb7bbVV5eXuN7vb29NXz4cLVv316//vqrkpKS1KVLF7Vu3drVZvv27Tp27JhS\nU1O1f/9+/e1vf9O0adMu4VAAAIBVuD2F1Lp1a+3YsaPCsp07d6pt27Y1vjcoKMg1mtK4cWOFh4cr\nPz+/QpuMjAz96U9/ks1mU4cOHXT27FkVFBS4Wx4AALAQt0dghg8fruTkZHXr1k0lJSV644039N13\n3ykxMfGSdpiTk6OffvpJV199dYXl+fn5Fe5qCgkJUX5+voKCgiq0S09PV3p6uiRpxowZlr0Tym63\nW/bYrYx+tx763Jro95q5HWA6dOigmTNnat26dfLz85PD4dC0adMUEhLi9s6Ki4uVkpKikSNHVnqG\nkmEYldpX9SDJ2NhYxcbGul7n5ua6vf/LicPhsOyxWxn9bj30uTVZtd/DwsLcbut2gJGk4OBg3XPP\nPTp16lSlkZGalJWVKSUlRX369FHPnj0rrQ8JCanQWXl5eZe8DwAAYA1uXwNz9uxZvf7663rkkUdc\nn76bkZGh999/v8b3GoahhQsXKjw8XAMHDqyyTVRUlNauXSvDMLRv3z75+/sTYAAAQJXcDjCLFy+W\nv7+/5s+fL7v9/MBNhw4dtHHjxhrf++OPP2rt2rXatWuXEhMTlZiYqMzMTK1evVqrV6+WJHXr1k2h\noaGKj4/XokWLNGbMmN95SAAA4HLn9hTSzp07tWjRIld4kaRmzZrp1KlTNb63Y8eOWr58ebVtbDYb\noQUAALjF7REYf39/FRYWVliWm5vLNA8AAPA4twNMTEyMUlJStGvXLtd1KvPmzdMdd9xRl/UBAABU\n4vYU0n333ScfHx8tWbJE5eXlWrBgQZWPBAAAAKhrbgcYm82mu+++W3fffXdd1gMAAFCjS/ocmOzs\nbB0+fFjFxcUVlkdHR9dqUQAAANVxO8B89NFH+vDDD3XllVeqUaNGFdYRYAAAgCe5HWA+//xzTZs2\nTVdeeWVd1gMAAFAjt+9C8vX1VXh4eF3WAgAA4Ba3A8zQoUP15ptvqqCgQE6ns8IfAAAAT3J7Cmn+\n/PmSpK+//rrSumXLltVeRQAAADVwO8DMnTu3LusAAABwm9sBpkWLFnVZBwAAgNvcvgYGAACgoSDA\nAAAA0yHAAAAA07nkAJObm6t9+/bVRS0AAABucfsi3tzcXL3++us6fPiwJOndd9/V5s2btWPHDo0d\nO7au6gMAAKjE7RGYN954Q926ddPbb78tu/187unSpYt++OGHOisOAACgKm4HmAMHDmjQoEHy8vr/\nb/H391dRUVGdFAYAAHAxbgeYwMBAHTt2rMKyI0eOyOFw1HpRAAAA1XH7Gph77rlHycnJGjRokJxO\np9avX6+PP/5YgwYNqsv6AAAAKnE7wERHRysgIEBff/21QkJC9O2332ro0KHq0aNHXdYHAABQidsB\nRpJ69OhBYAEAAPXukgLMnj179NNPP6m4uLjC8gceeKBWiwIAAKiO2wHmzTff1KZNm9SxY0f5+vq6\nlttstjopDAAA4GLcDjDr1q1TSkqKgoOD67IeAACAGrl9G7XD4ZCPj09d1gIAAOAWt0dgxo4dq0WL\nFql3794KDAyssK5z5861XhgAAMDFuB1gDh06pO3bt2vPnj0VroGRpAULFlT73vnz5yszM1OBgYFK\nSUmptD4rK0uvvPKKQkNDJUk9e/bUgw8+6G5pAADAYtwOMO+9954mTZqkLl26XPJO+vbtq/79+2ve\nvHkXbdOpUyclJSVd8rYBAID1uH0NTKNGjX73VFHnzp0VEBDwu94LAADwn9wegRk6dKjS0tL04IMP\nqlmzZhXW/fYBj7/Xvn37lJiYqKCgIA0fPlxt2rSpsl16errS09MlSTNmzLDss5jsdrtlj93K6Hfr\noc+tiX6vmc0wDMOdhkOHDr3oumXLltX4/pycHCUnJ1d5DUxRUZG8vLzk5+enzMxMpaWlKTU11Z2y\nlJ2d7Va7y43D4VBubm59lwEPo9+thz63Jqv2e1hYmNtt3R6BmTt37u8qxh3+/v6ur2+88UYtWbJE\np0+frjTSAwAAIF1CgGnRokWdFXHy5EkFBgbKZrPpwIEDcjqdatq0aZ3tDwAAmFu1AWbRokV68skn\nJUlz5sy56GMDnnrqqWp3Mnv2bO3evVuFhYUaO3ashgwZorKyMklSv379tHnzZq1evVre3t7y9fVV\nQkICjygAAAAXVW2AufC5LJLUqlWr372ThISEatf3799f/fv3/93bBwAA1lJtgLn//vu1fv163Xrr\nrRo8eLCnagIAAKhWjfc/L1682BN1AAAAuK3GAOPmXdYAAAAeU+NdSE6nU7t27aq2zXXXXVdrBQEA\nANSkxgBTWlqqhQsXXnQkxmaz1elnxAAAAPynGgOMn58fAQUAADQof/whRgAAAB7GRbwAAMB0agww\n77zzjifqAAAAcBtTSAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAA\nwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQIMAAAwHQI\nMAAAwHQIMAAAwHTsntjJ/PnzlZmZqcDAQKWkpFRabxiG3nrrLW3fvl2NGjVSXFyc2rdv74nSAACA\nCXlkBKZv3756/vnnL7p++/btOnbsmFJTU/XEE0/ob3/7myfKAgAAJuWRANO5c2cFBARcdH1GRob+\n9Kc/yWazqUOHDjp79qwKCgo8URoAADAhj0wh1SQ/P18Oh8P1OiQkRPn5+QoKCqrUNj09Xenp6ZKk\nGTNmVHifldjtdsseu5XR79ZDn1sT/V6zBhFgDMOotMxms1XZNjY2VrGxsa7Xubm5dVZXQ+ZwOCx7\n7FZGv1sPfW5NVu33sLAwt9s2iLuQQkJCKnRUXl5elaMvAAAAUgMJMFFRUVq7dq0Mw9C+ffvk7+9P\ngAEAABflkSmk2bNna/fu3SosLNTYsWM1ZMgQlZWVSZL69eunbt26KTMzU/Hx8fL19VVcXJwnygIA\nACblkQCTkJBQ7XqbzaYxY8Z4ohQAAHAZaBBTSAAAAJeCAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyH\nAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMA\nAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEyH\nAAMAAEyHAAMAAEyHAAMAAEyHAAMAAEzH7qkd7dixQ2+99ZacTqdiYmI0aNCgCuvXrFmjd999V8HB\nwZKk/v37KyYmxlPlAQAAE/FIgHE6nVqyZIn+/Oc/KyQkRM8995yioqLUunXrCu1uueUWjR492hMl\nAQAAE/PIFNKBAwfUqlUrtWzZUna7Xbfccou2bdvmiV0DAIDLkEdGYPLz8xUSEuJ6HRISov3791dq\nt2XLFu3Zs0dXXHGFRowYIYfDUalNenq60tPTJUkzZsyoso0V2O12yx67ldHv1kOfWxP9XjOPBBjD\nMCots9lsFV53795dvXv3lo+Pj1avXq158+ZpypQpld4XGxur2NhY1+vc3NzaL9gEHA6HZY/dyuh3\n66HPrcmq/R4WFuZ2W49MIYWEhCgvL8/1Oi8vT0FBQRXaNG3aVD4+PpLOh5RDhw55ojQAAGBCHgkw\nEREROnr0qHJyclRWVqaNGzcqKiqqQpuCggLX1xkZGZUu8AUAALjAI1NI3t7eeuyxx/Tyyy/L6XTq\n9ttvV5s2bbRs2TJFREQoKipKX3zxhTIyMuTt7a2AgADFxcV5ojQAAGBCNqOqC1RMJDs7u75LqBdW\nnR+1Ovrdeuhza7Jqvze4a2AAAABqEwEGAACYDgEGAACYDgEGAACYDgEGAACYDgEGAACYDgEGAACY\nDgEGAACYDgEGAACYDgEGAACYjkeehYSLS0lJ0auvvuqx/T3zzDOaMGGCx/YHAFbGOb7u8CykWvL2\ngN569qrm9V1GnXnl8EmN+HxDfZdheVZ9PoqV0ef173I/v0sN5xx/Kc9CIsDUkvDw8PouoU41b95c\nWVlZ9V2G5fHLzHro8/p3uZ/fpYZzjr+UAMMUUi3597//Xef7aNOmjQ4dOiQfHx/XSa20tFTt27fX\nL7/8Uuf7BwAr8sT5XeIcf6m4iNdEIiMjtXXr1grLtm7dqsjIyHqqCABQWzjHXxoCjInEx8dr4sSJ\n2rBhg0pLS7VhwwZNnDhR8fHx9V0aAOAP4hx/abgGxmRWrlyp1NRU7d+/X5GRkYqPj9egQYPquyx4\nCNdDWA99bi1WP8dzEa8FcFIWF1/8AAARc0lEQVSzJvrdeuhza7Jqv19KgGEKCQAAmA4BBgAAmA4B\nBgAAmA4BBgAAmA4BBgAAmA4BBgAAmA4BBjCBlStXKjo6Wo0bN1Z0dLRWrlxZ3yUBQL3iWUhAA7dy\n5UolJydr1qxZGjBggD7//HNNnDhRkiz1AVcA8FuMwAANXGpqqmbNmqXevXvLx8dHvXv31qxZs5Sa\nmlrfpaEOMeoGVI8RGKCB279/v3r06FFhWY8ePbR///56qgh1jVE3oGYeG4HZsWOHnn76aY0fP77K\n/0mUlpbqtdde0/jx4/X8888rJyfHU6UBDRpPqLUeRt2AmnkkwDidTi1ZskTPP/+8XnvtNW3YsEFH\njhyp0Oabb75RkyZNNGfOHN19991aunSpJ0oDGjyeUGs9jLoBNfNIgDlw4IBatWqlli1bym6365Zb\nbtG2bdsqtMnIyFDfvn0lSb169dKuXbtk8udMArVi0KBBmjRpkl544QU1a9ZML7zwgiZNmsRUwmWM\nUTegZh65BiY/P18hISGu1yEhIZX+J/HbNt7e3vL391dhYaGaNWtWoV16errS09MlSTNmzJDD4ajj\n6hsmu91u2WNvKF7ucqX+0rGVR/b1pKQnOzaWOnY5v2D14vN/6tjUvcc0+Yd/1fl+zMQT/Z7VsbG0\ndJr0/waiW0p6QNIDHRtLcXfX6b7p84aBc3zNPBJgqhpJsdlsl9xGkmJjYxUbG+t6bcXHjUvWfdR6\nQ/LkN9/puIf36el+f1LW/Rm7GE/1+8qVK5Wamqr9+/crMjJS8fHxHhl1o88bBque48PCwtxu65EA\nExISory8PNfrvLw8BQUFVdkmJCRE5eXlKioqUkBAgCfKA4AGZ9CgQRo0aJBlf5EBNfHINTARERE6\nevSocnJyVFZWpo0bNyoqKqpCm+7du2vNmjWSpM2bN+vaa6+tcgQGAADAIyMw3t7eeuyxx/Tyyy/L\n6XTq9ttvV5s2bbRs2TJFREQoKipK0dHRmjt3rsaPH6+AgAAlJCR4ojQAAGBCNsPkt/pkZ2fXdwn1\ngmFla6LfrYc+tyar9vulXAPDowQAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAAAIDpEGAA\nAIDpEGAAAIDpEGAAAIDpmP6TeAEAgPUwAmNSSUlJ9V0C6gH9bj30uTXR7zUjwAAAANMhwAAAANMh\nwJhUbGxsfZeAekC/Ww99bk30e824iBcAAJgOIzAAAMB07PVdwOVs6NChatu2rcrLy+Xt7a3bbrtN\nAwYMkJfXxXNjTk6O9u3bp1tvvfV37XPNmjXq0qWLgoODJUkLFy7UwIED1bp169+1PVTvQh87nU6F\nh4dr3LhxatSoUX2XVen76ODBg/r222/12GOP1XNl5nPy5EmlpaXp4MGDstvtCg0N1YgRI5SSkqKU\nlJRa2ceyZcvUqVMndenSRXv27NHixYvl7e2t5557Tm+99ZYmTJhwydvkXNBwFBYWaurUqZLOfz95\neXmpWbNmkqTp06fLbq/4q/jMmTPauHGj+vXrV+12y8vLNXr0aKWlpdVJ3Q0dAaYO+fr6aubMmZKk\nU6dOKTU1VUVFRRoyZMhF33PixAmtX7/+DwWYNm3auE5aY8eO/V3bgXt+28epqan66quvNHDgQNd6\nwzBkGEa1ofX3uhCMq/Kf30cRERGKiIio9Roud4ZhaObMmbrtttuUkJAgSTp8+LBOnTpVq/sZOnSo\n6+t169bpnnvu0e233y5Jvyu8SJwLGpKmTZu6zhPLly+Xn5+f7r333ou2P3PmjL766qsaA4zVEWA8\nJDAwUE888YSee+45DR48WIZhaOnSpdq9e7dKS0t155136o477tD//M//6MiRI0pMTHSN2FTVTpI+\n+eQTrV27Vl5eXuratasiIiJ08OBBpaamytfXVy+//LKmTZum4cOHKyIiQuvXr9fHH38sSerWrZse\nffRRSdLw4cM1YMAAZWZmytfXV4mJiWrevHm9/VuZVceOHfXzzz8rJydH06dP17XXXqt9+/YpMTFR\n2dnZWr58ucrKytSyZUvFxcXJz89P48aN080336ysrCxJ0tNPP61WrVopIyNDH330kcrKytS0aVON\nHz9ezZs31/Lly1VQUKATJ06oadOmeuihhzR37lydO3dOkvTYY4/pmmuuqfR91K5dO61atUpJSUk6\nc+aM5s+fr5ycHDVq1EhPPPGErrzySi1fvly5ubnKyclRbm6uBgwYoAEDBtTnP2m9y8rKkt1ur/CL\n5KqrrlJOTo7rdU5OTpV9UFBQoNmzZ6uoqEhOp1NjxozRNddcowULFujQoUOSpNtvv10DBw7UvHnz\n1L17d509e1abNm3S999/r507d2rYsGFKTk5WSkqKnE6n/v73v+v777+XzWZTTEyM7rrrLq1YsULf\nffedSkpK1KFDBz3xxBPasmUL5wKTuHAel85fuHvXXXdp6dKlys7OVmJiorp27ar7779fM2fOVFFR\nkcrLy/XQQw+pe/fu9Vx5A2Cgzjz66KOVlo0cOdIoKCgwvvrqK2PFihWGYRhGSUmJMWnSJOP48ePG\nrl27jOnTp7vaX6xdZmamMXnyZKO4uNgwDMMoLCw0DMMwpkyZYhw4cMD1/guv8/LyjLFjxxqnTp0y\nysrKjP/zf/6PsWXLFsMwDGPw4MHGtm3bDMMwjHfffde1P9TsQh+XlZUZycnJxpdffmkcP37cGDJk\niPHjjz8ahmEYp06dMv7yl78Yv/76q2EYhvHxxx8bH3zwgWEYhhEXF2d8+OGHhmEYxpo1a1x9X1hY\naDidTsMwDCM9Pd14++23DcMwjGXLlhnPPvusce7cOcMwDKO4uNj1dXZ2tjFp0iTDMIxK30e/fb1k\nyRJj+fLlhmEYxs6dO42JEye6tj158mSjpKTEOHXqlDFq1CijtLS01v/NzOSzzz4z3nrrrUrLjx8/\nbjzzzDOGYVy8D/7xj3+4+ra8vNwoKioyDh48aEydOtW1nTNnzhiGYRhz5841Nm3aVOnr3+7nyy+/\nNGbOnGmUlZUZhvH/f+Yv/G0YhpGamur6WeZc0DAtW7bM+OSTTwzDMIz9+/cbEydONIqLi42ioiIj\nISHBOHz4sHH06FHXz6VhGEZpaalRVFRkGIZhnDx50hg/frxhGOfPOyNGjPD4MTQUjMB4mPH/bvr6\n/vvv9fPPP2vz5s2SpKKiIh09erTSXOjF2u3cuVN9+/Z1XW8REBBQ7X4PHjyoa6+91jXv2qdPH+3Z\ns0c9evSQ3W53pfn27dvrhx9+qL0DvsyVlJQoMTFRktSpUydFR0crPz9fDodDHTp0kCTt379fR44c\n0QsvvCBJKisrc62TpN69e7v+fvvttyVJ+fn5mj17tgoKClRWVqbQ0FBX+6ioKPn6+ko6P420ZMkS\nHT58WF5eXjp69GiNNe/du9c1LXHdddfpzJkzKioqkiTdeOON8vHxkY+PjwIDA3Xq1CmFhIT8oX+j\ny93F+iAiIkILFixQWVmZevTooauuukqhoaHKycnRm2++qRtvvFFdunRxez8//PCD+vXr55o2vPAz\nv2vXLv3jH//QuXPndObMGbVp00ZRUVEX3Q7ngoZjz5496tmzp+s8ftNNN2nv3r264YYbKrVdunSp\n9u7dK5vNpry8PJ0+fVpNmjTxdMkNCgHGg44fPy4vLy8FBgbKMAyNGjVKXbt2rdDmwlTCBRdrt2PH\nDtlsNrf3bVRzt7y3t7drW15eXiovL3d7u1b322tgfsvPz8/1tWEYuv76613XUPyn3/bjha/ffPNN\nDRw4UFFRUcrKytIHH3zgavPbi4Q//fRTBQYGaubMmTIMQ4888kiNNVf3vfDbAM33gtSmTRtt2bKl\n2jYX64POnTvrxRdfVGZmpubMmaN7771Xt912m2bOnKkdO3bon//8pzZu3Ki4uLjfXV9JSYmWLFmi\n6dOny+FwaPny5SopKan2PZwLGo7q+uK3vv32WxUVFSk5OVne3t4aO3asSktL67i6ho/bqD3k9OnT\nWrx4sfr37y+bzaauXbtq9erVKisrkyRlZ2eruLhYjRs31q+//up638Xa3XDDDfrf//1f17z7mTNn\nJJ3/xfnb918QGRmp3bt36/Tp03I6ndqwYYM6d+5c14cNSR06dNCPP/6oY8eOSZLOnTun7Oxs1/qN\nGze6/o6MjJR0fqTtwsWX33777UW3XVRUpKCgIHl5eWnt2rVyOp2SVOn76Lc6deqkdevWSTofmJs2\nbSp/f/8/eJSXp+uuu06lpaVKT093LTtw4IByc3Ndry/WBydOnFBgYKBiY2MVHR2tn376yfXz16tX\nLw0bNkw//fST27V06dJFX331lStUnDlzxvVLrFmzZiouLq4QtjgXNHydO3fW1q1bVVJSouLiYm3b\ntk2dOnWSn5+fiouLXe2KiorUrFkzeXt764cfflB+fn49Vt1wMAJThy5ML1y4W6RPnz6uO1Sio6OV\nk5OjSZMmSTp/AkpMTFTbtm3l7e1d4SLeqtp17dpVhw8fVlJSkux2u7p166aHH35Yffv21eLFi10X\n7l0QFBSkhx9+WC+++KKk8xfu3XTTTR7+F7GmZs2aady4cXr99dddv3CGDRumsLAwSVJpaamef/55\nGYahp59+WpI0ePBgvfrqqwoODlZkZGSFi0Z/684771RKSoo2b96sa6+91jU685/fR+3atXO9Z8iQ\nIZo/f74mTpyoRo0aady4cXV5+KZms9k0ceJEpaWl6ZNPPpGPj49atGihkSNHutpcrA+ysrK0atUq\neXt7y8/PT0899ZTy8/O1YMECV8h5+OGH3a4lJiZGR48e1cSJE2W32xUTE6P+/fsrJiZGEyZMUGho\naIU7zTgXNHxXX321evfureeee06S1K9fP7Vt21bS+Sm8CRMm6MYbb9TAgQOVnJyspKQktWvXTldc\ncUV9lt1g8Em8QD0aN26cpk+f7roeAQDgHqaQAACA6TACAwAATIcRGAAAYDoEGAAAYDoEGAAAYDoE\nGAAAYDp8DgwAj9u7d6/+/ve/65dffpGXl5dat26tESNG6MiRI/r666/10ksv1XeJABo4AgwAjyoq\nKtKMGTM0ZswY3XLLLSorK9OePXvk4+NT36UBMBECDACPuvCww1tvvVXS+edJ3XDDDTpy5IgWL16s\nsrIyDR8+XN7e3kpLS1Npaanee+89bdq0SWVlZbrppps0cuRI+fr6KisrS3PmzFG/fv302Wefyc/P\nT8OGDVOfPn3q8xABeADXwADwqCuuuEJeXl6aO3eutm/f7nqOV+vWrfX444+rQ4cOevfdd5WWlibp\n/FN4jx49qpkzZyo1NVX5+flasWKFa3snT55UYWGhFi5cqHHjxumNN96o8KwpAJcnAgwAj/L399fU\nqVNls9m0aNEijRkzRsnJyTp58mSltoZh6Ouvv9aIESMUEBCgxo0b64EHHtCGDRsqtBs6dKh8fHzU\nuXNndevWzfWATACXL6aQAHhc69atXQ+R/Pe//605c+YoLS1NXbt2rdDu9OnTOnfunJKSklzLDMNw\nPQxRkpo0aSI/Pz/X6xYtWqigoKCOjwBAfSPAAKhX4eHh6tu3r7766qtKAaZp06by9fV1PZm7KmfP\nnlVxcbErxOTm5qpNmzZ1XjeA+sUUEgCP+ve//61Vq1YpLy9P0vnAsWHDBkVGRqp58+bKz89XWVmZ\nJMnLy0sxMTFKS0vTqVOnJEn5+fnasWNHhW0uX77cdTdTZmambr75Zs8eFACPYwQGgEc1btxY+/fv\n16effqqioiL5+/ure/fuevTRR+Xr6+u6mNfLy0tLlizRI488ohUrVmjy5MkqLCxUcHCw7rjjDtdo\nTfPmzRUQEKAnn3xSvr6+evzxxxUeHl7PRwmgrvE0agCmdeE26oULF9Z3KQA8jCkkAABgOgQYAABg\nOkwhAQAA02EEBgAAmA4BBgAAmA4BBgAAmA4BBgAAmA4BBgAAmA4BBgAAmM7/BcEYtAYEsgOLAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2208dff41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#time_deltas = np.array(delta_times_total)\n",
    "# Create a figure instance\n",
    "fig = plt.figure(1, figsize=(9, 6))\n",
    "\n",
    "# Create an axes instance\n",
    "ax = fig.add_subplot(111)\n",
    "plt.title(\"Timings\")\n",
    "plt.ylabel('Time needed [s]')\n",
    "plt.xlabel('Step')\n",
    "#plt.ylim([0.000997,0.001005])\n",
    "plt.style.use('fivethirtyeight')\n",
    "bp = ax.boxplot(time_deltas)\n",
    "\n",
    "labels = [\"Detection\", \"Preparation\", \"Classification\", \"Total\"]\n",
    "plt.xticks([i+1 for i in range(len(labels))], labels)\n",
    "np.median(time_deltas, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
